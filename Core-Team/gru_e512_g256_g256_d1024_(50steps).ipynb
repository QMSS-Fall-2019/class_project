{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gru_e512_g256_g256_d1024_(50steps)",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKkKtD12Ejha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "e046ebbe-3236-4a23-ab8c-f45911d1a45f"
      },
      "source": [
        "# ! pip install -U tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 34kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giT-5ASe_C8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E32KxzGEkz-",
        "colab_type": "code",
        "outputId": "982ac611-20cc-4618-ec0c-f7880171129c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLkgP6__HIq",
        "colab_type": "code",
        "outputId": "6f3640cd-5522-40d8-f309-a4d00376cfc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK-iruJ0_j67",
        "colab_type": "code",
        "outputId": "dbb58c03-b13c-41e9-9969-8e56d5897947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "master_df = pd.read_csv('/content/drive/My Drive/Consumer_Complaints.csv')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,6,11,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSUqa1mmTF4N",
        "colab_type": "text"
      },
      "source": [
        "## Sample and Process Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx6xIRwk_w3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "proc_df = master_df.dropna(subset=['Consumer complaint narrative']).sample(20000, random_state=55)\n",
        "\n",
        "clean = [re.sub('[^A-Za-z.,\\s\\']', '', nar) for nar in proc_df['Consumer complaint narrative']]\n",
        "\n",
        "split_word_nars = [nar.split() for nar in clean]\n",
        "\n",
        "\"\"\"Contractions Import\"\"\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')\n",
        "from english_contractions import replace_contraction\n",
        "\n",
        "\"\"\"End Contraction Import\"\"\"\n",
        "\n",
        "new_words = []\n",
        "\n",
        "for nar in split_word_nars:\n",
        "\n",
        "  nar_words = []\n",
        "\n",
        "  for word in nar:\n",
        "\n",
        "    if re.search('\\w+[.]', word):\n",
        "\n",
        "      splitted = word.split('.')\n",
        "\n",
        "      tmp_words = replace_contraction(splitted[0].lower())\n",
        "\n",
        "      for w in tmp_words.split():\n",
        "\n",
        "        nar_words.append(w)\n",
        "\n",
        "      nar_words.append('.')\n",
        "    \n",
        "    elif re.search('\\w+[,]', word):\n",
        "      \n",
        "      splitted = word.split(',')\n",
        "\n",
        "      tmp_words = replace_contraction(splitted[0].lower())\n",
        "\n",
        "      for w in tmp_words.split():\n",
        "\n",
        "        nar_words.append(w)\n",
        "\n",
        "      nar_words.append(',')\n",
        "    \n",
        "    elif re.match('[.]', word):\n",
        "      \n",
        "      placeholder = 1\n",
        "    \n",
        "    else:\n",
        "\n",
        "      tmp_words = replace_contraction(word)\n",
        "\n",
        "      for w in tmp_words.split():\n",
        "\n",
        "        nar_words.append(w)\n",
        "\n",
        "  new_words.append(nar_words)\n",
        "\n",
        "words = [word for words in new_words for word in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9cKM19tTLJi",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAfI4Onw_3ir",
        "colab_type": "code",
        "outputId": "97f37381-70cb-4ce2-904b-bd3d828238bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, filters='')\n",
        "\n",
        "tokenizer.fit_on_texts(words)\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "max_id = num_words\n",
        "\n",
        "dataset_size = tokenizer.document_count\n",
        "\n",
        "flat_encoded = [enc for encoder in encoded for enc in encoder]\n",
        "\n",
        "print(max_id)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5-GJJrXTOdM",
        "colab_type": "text"
      },
      "source": [
        "## Split Train and Val Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZyo8gh_6Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[:train_size])\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(flat_encoded[train_size:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YBzSUUkTQQO",
        "colab_type": "text"
      },
      "source": [
        "## Batch and Prepare Train Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWS2Mzb_8A0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "n_steps = 50\n",
        "\n",
        "window_length = n_steps + 1\n",
        "\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "# dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7hNzKoaTT_t",
        "colab_type": "text"
      },
      "source": [
        "## Batch and Prepare Val Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ11iYm8__B2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = val_dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "val_dataset = val_dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "val_dataset = val_dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "val_dataset = val_dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "# val_dataset = val_dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
        "\n",
        "val_dataset = val_dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfXB6GSUOWqO",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW1kZCD4zqlM",
        "colab_type": "code",
        "outputId": "ca9b313b-40ff-46c3-9f43-b0f18280f333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Early Stopping\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopper = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "# Checkpoing Model Weights\n",
        "\n",
        "import os\n",
        "\n",
        "checkpoint_path = 'checkpoints/cp-{epoch:04d}.ckpt'\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    period=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHMHrt4zTWze",
        "colab_type": "text"
      },
      "source": [
        "## Identify Last Epoch Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJrFnmyElWXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b56e29d-00c4-46bb-96c0-fc7296bb9b16"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "latest"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'checkpoints/cp-0002.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mVvv3J4ThZi",
        "colab_type": "text"
      },
      "source": [
        "## Size of Train Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oESe7TX5ABaF",
        "colab_type": "code",
        "outputId": "21991c76-0ab3-43b0-c09a-7a565dff8f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "steps_per_epoch = train_size // batch_size\n",
        "\n",
        "steps_per_epoch"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902P8Fq3TkTZ",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGHvprrVADSD",
        "colab_type": "code",
        "outputId": "a00836a5-3b1e-4b17-f852-be52926ae334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "adam = tf.keras.optimizers.Adam()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Embedding(max_id, 512, input_shape=[None]),\n",
        "                                    tf.keras.layers.GRU(256, return_sequences=True,\n",
        "                                                        dropout=0.5, recurrent_dropout=0.5),\n",
        "                                    tf.keras.layers.GRU(256, return_sequences=True,\n",
        "                                                        dropout=0.5, recurrent_dropout=0.5),\n",
        "                                    tf.keras.layers.Dropout(0.5),\n",
        "                                    tf.keras.layers.Dense(1024, activation='sigmoid'),\n",
        "                                    tf.keras.layers.Dense(max_id, activation='softmax')])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(latest)\n",
        "\n",
        "model.fit(dataset, epochs=5, validation_data=val_dataset, callbacks=[early_stopper, checkpoint_cb])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  14156/Unknown - 5793s 409ms/step - loss: 3.8019 - accuracy: 0.2697\n",
            "Epoch 00001: saving model to checkpoints/cp-0001.ckpt\n",
            "14156/14156 [==============================] - 5973s 422ms/step - loss: 3.8019 - accuracy: 0.2697 - val_loss: 4.3173 - val_accuracy: 0.2516\n",
            "Epoch 2/5\n",
            "14155/14156 [============================>.] - ETA: 0s - loss: 3.5843 - accuracy: 0.2867\n",
            "Epoch 00002: saving model to checkpoints/cp-0002.ckpt\n",
            "14156/14156 [==============================] - 5967s 422ms/step - loss: 3.5842 - accuracy: 0.2867 - val_loss: 4.2975 - val_accuracy: 0.2528\n",
            "Epoch 3/5\n",
            "14155/14156 [============================>.] - ETA: 0s - loss: 3.5088 - accuracy: 0.2914\n",
            "Epoch 00003: saving model to checkpoints/cp-0003.ckpt\n",
            "14156/14156 [==============================] - 5978s 422ms/step - loss: 3.5087 - accuracy: 0.2914 - val_loss: 4.3077 - val_accuracy: 0.2534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63a6297780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkbiSXd7kWu8",
        "colab_type": "code",
        "outputId": "069152c5-6d02-49a8-b08c-5cca9562f40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_30 (GRU)                 (None, None, 256)         7878144   \n",
            "_________________________________________________________________\n",
            "gru_31 (GRU)                 (None, None, 256)         394752    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, None, 256)         65792     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, None, 10000)       2570000   \n",
            "=================================================================\n",
            "Total params: 10,908,688\n",
            "Trainable params: 10,908,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnKX_z9JhwIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha6YqnWwc4Gi",
        "colab_type": "text"
      },
      "source": [
        "## Save the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-aAO0TvdTTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_path = '/content/drive/'\n",
        "\n",
        "file_path = 'My Drive/saved_keras_rnns/'\n",
        "\n",
        "name = 'gru_1001.h5'\n",
        "\n",
        "model.save(drive_path + file_path + name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TKPz18_A2k7",
        "colab_type": "text"
      },
      "source": [
        "## Save the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpny2hXlA4W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "tokenizer_name = 'tok_gru_1001.pkl'\n",
        "\n",
        "pickle.dump(tokenizer, open(drive_path + file_path + tokenizer_name, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfRRUk4EywYL",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpsZDDXMePG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = 'gru_30_steps.h5'\n",
        "\n",
        "new_model = keras.models.load_model(drive_path + file_path + name)\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_DB73L-708y",
        "colab_type": "text"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaW0U15tAhii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_clean(text):\n",
        "\n",
        "  splits = text.split(' . ')[:-1]\n",
        "\n",
        "  new_splits = []\n",
        "\n",
        "  for split in splits[1:]:\n",
        "\n",
        "    word_splits = split.split(' ')\n",
        "    \n",
        "    word_splits[0] = word_splits[0].capitalize()\n",
        "    \n",
        "    word_splits[-1] = ''.join([word_splits[-1], '.'])\n",
        "\n",
        "    joined = ' '.join(word_splits)\n",
        "\n",
        "    new_splits.append(joined)\n",
        "  \n",
        "  join_split = ' '.join(new_splits)\n",
        "\n",
        "  return join_split\n",
        "\n",
        "def preprocessor(text):\n",
        "\n",
        "  # X = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "  # return tf.one_hot(X, max_id) # for no embedding\n",
        "\n",
        "  # return tokenizer.texts_to_sequences(text) # for embedding\n",
        "\n",
        "def next_word(text, model, temperature=1):\n",
        "  \n",
        "  X_new = preprocessor([text])\n",
        "\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "\n",
        "  word_id = tf.random.categorical(rescaled_logits, num_samples=1)\n",
        "\n",
        "  return tokenizer.sequences_to_texts(word_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, model, n_words=50, temperature=0.5):\n",
        "\n",
        "  for _ in range(n_words):\n",
        "\n",
        "    space = [' ', next_word(text, model, temperature)]\n",
        "    \n",
        "    text += ''.join(space)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sITGZHtLZfXT",
        "colab_type": "code",
        "outputId": "8ff87c58-0f0b-4561-e57b-b9766655d0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "key_words = ['fcra', 'loan', 'bank', 'equifax breach', 'credit card', 'credit report']\n",
        "\n",
        "temps = [0.15, 0.2, 0.25]\n",
        "\n",
        "count = 0\n",
        "\n",
        "for key_word in key_words:\n",
        "  for temp in temps:\n",
        "    count += 1\n",
        "    print(f'{count} (kw: \"{key_word}\", tmp: {temp})', text_clean(complete_text(key_word, new_model_2, temperature=temp)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 (kw: \"fcra\", tmp: 0.15) I have been disputing this debt with transunion and i have never received any response from the provider of service. I have been disputing the pmi with the credit bureaus. I have been disputing this account with the credit bureaus.\n",
            "2 (kw: \"fcra\", tmp: 0.2) I have been disputing this account with bsi financial services. I have been disputing this account with my credit report. I have been disputing this debt with the credit bureaus. I have been disputing this debt from my credit report.\n",
            "3 (kw: \"fcra\", tmp: 0.25) I have contacted them about the process of the phone with the company and that my credit score is now. I have been trying to obtain a home and they have not been able to do so. I am not sure what happened to the.\n",
            "4 (kw: \"loan\", tmp: 0.15) I was told that the check was rejected by seterus,. Inc. I was told that the check had bounced. I was told that the check was rejected by the bank of america.\n",
            "5 (kw: \"loan\", tmp: 0.2) I have been a victim of identity theft and i have never received any response from the provider of service. I have been disputing this debt with my credit report. I have been disputing this debt with the credit bureaus.\n",
            "6 (kw: \"loan\", tmp: 0.25) I was told that i was a victim of identity theft. I have been disputing this account with my credit report. I have been disputing this debt on my credit report and i have never received a response from the company.\n",
            "7 (kw: \"bank\", tmp: 0.15) I was told that the check was rejected by the bank of america. I was told that the check had bounced. I was told that the check was not mine.\n",
            "8 (kw: \"bank\", tmp: 0.2) I was told that the check was rejected by the bank of america. I was told that the payment was rejected by the bank of america. I was told that the check was accepted and i was told that the check was rejected.\n",
            "9 (kw: \"bank\", tmp: 0.25) I was told that the check was rejected by the bank of america. I was told that the check was rejected by the bank of america preferred line and i was told that the check was rejected.\n",
            "10 (kw: \"equifax breach\", tmp: 0.15) I am requesting immediate removal of the debt. I have been disputing this account with my credit report and i have been disputing this debt. I have been disputing this account with transunion.\n",
            "11 (kw: \"equifax breach\", tmp: 0.2) I have been disputing this account with my credit report. I have been disputing this account with the credit bureaus. I have been disputing this debt from my credit report.\n",
            "12 (kw: \"equifax breach\", tmp: 0.25) I have been disputing the pmi with the company that i am not able to obtain a new credit card holder. I have never received any communication from the company. I have been disputing the pmi with.\n",
            "13 (kw: \"credit card\", tmp: 0.15) But he was going to collect a debt. I was told that the check was rejected by the bank. I was told that the check was rejected by the bank. I was told that the check was rejected by the bank of america.\n",
            "14 (kw: \"credit card\", tmp: 0.2) I have been disputing the pmi. I have been disputing this account with. I have never received anything from the company. I have no idea what i can do with this company.\n",
            "15 (kw: \"credit card\", tmp: 0.25) I have been trying to get a letter from the company. I have never been able to get a response from them. I have been disputing this account with my credit report.\n",
            "16 (kw: \"credit report\", tmp: 0.15) I have been disputing the pmi. I have been disputing this account with the credit bureaus. I have been disputing this debt from my credit report. I have never been able to obtain a new credit report.\n",
            "17 (kw: \"credit report\", tmp: 0.2) I have never received any response from the provider of service. I have been disputing the pmi with the company and they have refused to remove this account.\n",
            "18 (kw: \"credit report\", tmp: 0.25) I have been disputing this account with them. I have been disputing the pmi. I have been trying to resolve this issue with the company to remove this account. I have been trying to resolve this issue.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}